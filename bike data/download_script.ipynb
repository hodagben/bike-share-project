{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583c1205-fe0a-459f-a4bb-cd5ee7d4a924",
   "metadata": {},
   "source": [
    "# Downloading Mobi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aee8ce-ebb5-4ffd-ad4a-9476f434cc46",
   "metadata": {},
   "source": [
    "## Accessing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a751f1e-1a32-4e09-8155-ce238777d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # for html parsing\n",
    "import requests # for accessing html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac3e6a-547b-4962-913c-a7dbcedcca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Mobi site\n",
    "response = requests.get('https://www.mobibikes.ca/en/system-data')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbc340-f9ba-4349-834f-a25d46ccbfe4",
   "metadata": {},
   "source": [
    "The data urls all include 'google.com'.  We can use this to filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc5ee2-9ccb-4480-9601-847f295d00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create html parser\n",
    "soup = BeautifulSoup(markup=response.text, features='html.parser')\n",
    "data_elements = soup.find_all(href = lambda x : x and 'google.com' in x)\n",
    "data_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075f2b-1aed-4c1d-b6c7-1d01a48e32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary of urls and their text labels (dates)\n",
    "urls = {elt.get_text() : elt['href'] for elt in data_elements}\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4df9a-1b7f-45bc-b84d-852bf21f423b",
   "metadata": {},
   "source": [
    "There is a little bit of cleanup to do:\n",
    "* July 2019 is split into two entries (`'Ju'` and `'ly 2019'`) with the same URL.\n",
    "* The penultimate entry of `urls` (`'\\xa0'`) is a repeat of 2017 data (I think?).\n",
    "* The final entry of `urls` (`'https://...'`) is a repeat of April 2022.  (Tbh I don't know why this was here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd09a6-57cd-4eb6-a51a-a760d55bbd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename 'Ju' to 'July 2019'\n",
    "urls['July 2019'] = urls.pop('Ju')\n",
    "\n",
    "# delete duplicates\n",
    "for key in ['ly 2019', '\\xa0', 'https://drive.google.com/file/d/1c_rWhMcwXRnNt06psxmt_UN8IjbJF91E/view?usp=sharing']:\n",
    "    del urls[key]\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d4414-80d8-44d0-ad03-e62af4619809",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4953385-e251-4bc6-a7bf-a61b88145caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct URL for downloading (rather than viewing) data\n",
    "\n",
    "def get_download_url(date):\n",
    "    url = urls[date]\n",
    "    file_ID = url.split(sep='/')[-2]\n",
    "\n",
    "    if 'drive.google.com' in url:\n",
    "        return 'https://drive.google.com/uc?export=download&id=' + file_ID\n",
    "    elif 'docs.google.com' in url:\n",
    "        return 'https://docs.google.com/uc?export=download&id=' + file_ID\n",
    "    else:\n",
    "        print('Unable to constuct URL for ' + date + ' data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084df976-3ffe-4d44-8bf1-e3abfe08790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data for 'dates', a list of keys for the urls dictionary\n",
    "\n",
    "import os\n",
    "\n",
    "def download(dates):\n",
    "    if not isinstance(dates,list):\n",
    "        dates = [dates]\n",
    "    for date in dates:\n",
    "        if os.path.exists(date+'.csv'):\n",
    "            print('File ' + date + '.csv already exists.  No download initiated')\n",
    "        else:\n",
    "            url = get_download_url(date)\n",
    "            response = requests.get(url)\n",
    "            code = response.status_code\n",
    "            if code != 200:\n",
    "                print('Unable to use URL for ' + date + ' data (error code ' + str(code) + '): ' + url)\n",
    "            else:\n",
    "                with open(date+'.csv', 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print('Data for ' + date + ' written to file ' + date + '.csv')\n",
    "            response.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241f168-1a6f-4c8d-88e4-260093ea3cda",
   "metadata": {},
   "source": [
    "To download data, call the `download` function.  The argument `dates` should be a key of the dictionary `urls` (e.g. `'April 2024'`) or a list of keys (e.g. `['April 2024, 'June 2021']`).  To download all of the data, you can use `dates = list(urls.keys())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283fa7e-090b-4680-ad8a-4535085d75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
